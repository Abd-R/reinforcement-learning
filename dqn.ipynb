{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, targets_f = [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward     # Target = 1 OR -10\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *     # New Q value [state, action] = Current Reward + decay * next best action\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target            # Target[left/right] = New Q value\n",
    "            # Filtering out states and targets for training\n",
    "            states.append(state[0])                 # Input State for New Q value\n",
    "            targets_f.append(target_f[0])           # [1, 2, 3, 4] -> [target, arbitrary]\n",
    "\n",
    "        history = self.model.fit(np.array(states), np.array(targets_f), epochs=1, verbose=1)\n",
    "        # Keeping track of loss\n",
    "        loss = history.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    state_size  : 4 : (Cart Position, Cart Velocity, Angle of Pole, Pole's Angular Velocity)\n",
    "    action_size : 2 : (Left or Right) (0, 1)\n",
    "\"\"\"\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "done = False\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "1. For Each Episode, we are iterating 500 times. \n",
    "2. For each time, \n",
    "3. We calculate Q Values for Input States\n",
    "4. Input State becomes Features, Actions become targets\n",
    "5. Q value calucated by Bellman Equation becomes target for this exploration or explitation timestep\n",
    "6. If the memory sample increases batch size, we train the model on memory data\n",
    "7. For first model.replay, no. of examples = 32, for 2nd, 32* 3, and so on\n",
    "8. The Q-value for a state-action pair (s, a) represents the expected cumulative reward when taking action a in state s \n",
    "9. Neural network is trained to minimize the error between Predicted Actopms and target Q-values.\n",
    "10. After training, model learns to map the action to states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset() \n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "agent.act(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    state = env.reset() \n",
    "    state = np.reshape(state[0], [1, state_size])  # Probability Distribution (1, 4)\n",
    "    for time in range(500):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # -10 provides a strong disincentive to the agent to end an episode early\n",
    "        # by losing control of balancing the pole or navigating off the screen                                                   \n",
    "        reward = reward if not done else -10        \n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                    .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            loss = agent.replay(batch_size)\n",
    "            # Logging training loss every 10 timesteps\n",
    "            if time % 10 == 0:\n",
    "                print(\"episode: {}/{}, time: {}, loss: {:.4f}\"\n",
    "                    .format(e, EPISODES, time, loss))  \n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(\"./save/cartpole-dqn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
